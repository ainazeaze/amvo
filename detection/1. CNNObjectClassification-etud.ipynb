{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b0ef3e-f95e-4919-88ef-13a60b9a9ebf",
   "metadata": {},
   "source": [
    "# Exercice 1. CNN for object classification\n",
    "\n",
    "Le jeu de données Toy Dataset - v7 2023-04-19 1:21am est fourni par Roboflow (https://universe.roboflow.com/tips-wis6y/toy-dataset-bfq0d). Vous en trouverez une copie sur la page Moodle.\n",
    "\n",
    "Les données brutes se trouvent dans le sous-dossier « images ». Les données d'étiquetage se trouvent dans le sous-dossier « labels ».\n",
    "\n",
    "Le format des données d'étiquetage est le suivant :\n",
    "\n",
    "    categ, cx, cy, w, h\n",
    "\n",
    "où categ représente le numéro de catégorie, cx et cy les coordonnées du centre de l'objet (sur une échelle de 0 à 1), et w et h ses valeurs horizontale et verticale (exprimées en pourcentage de la largeur et de la hauteur). (cx, cy, w, h) constitue le cadre englobant de l'objet. Cette représentation est conforme au format YOLO8.\n",
    "\n",
    "Afin d'exploiter tous les objets présents dans les images, lors du chargement du jeu de données, nous pouvons extraire les images des objets à partir des images originales en les recadrant à l'intérieur de leur cadre englobant.\n",
    "\n",
    "Deux configurations sont présentées ci-dessous :\n",
    "\n",
    "    all objects versus all objects (load_objects)\n",
    "    one object versus all (load_objects_as_binary_problem)\n",
    "\n",
    "**Q1** Vous pouvez utiliser le code tel quel, mais vous pouvez également le modifier afin d'obtenir un équilibre entre les classes, notamment pour la classification binaire. Pourquoi pensez-vous que cela peut être intéressant d'équilibrer entre les classes ?\n",
    "\n",
    "__Réponse__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e881a6c-9c36-43ea-a8cf-5da03fe067ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "train_path=\"../toy-dataset/train/\"\n",
    "test_path=\"../toy-dataset/test/\"\n",
    "\n",
    "def load_objects(imgs_path,max_samples=np.iinfo(np.int64)):\n",
    "    y_obj=[]\n",
    "    x_obj=[]\n",
    "    imgs_files = os.listdir( imgs_path+\"images/\" )\n",
    "    for i,img_file in enumerate(imgs_files):\n",
    "        if (len(x_obj)==max_samples):\n",
    "            break\n",
    "        label_file=img_file[:-4]+\".txt\"\n",
    "        img_init=cv2.imread(imgs_path+\"images/\"+img_file)\n",
    "        labels=csv.reader(open(imgs_path+\"labels/\"+label_file,\"r\"),delimiter=' ')\n",
    "        rows = list(labels)\n",
    "        if len(rows)>0:\n",
    "            for j,row in enumerate(rows):\n",
    "                if (len(x_obj)==max_samples):\n",
    "                    break\n",
    "                y_obj.append(row[0])\n",
    "                bbox=np.array(row[1:],dtype=np.float32) if row[0]==1 else np.array(row[1:],dtype=np.float32)\n",
    "                w=int(bbox[2]*img_init.shape[0]*2)\n",
    "                h=int(bbox[3]*img_init.shape[1]*2)\n",
    "                x0=max(0,int(math.trunc(np.float64(bbox[0]*img_init.shape[0]-w/2))))\n",
    "                y0=max(0,int(math.trunc(np.float64(bbox[1]*img_init.shape[1]-h/2))))\n",
    "                x1=min(img_init.shape[0],int(math.trunc(np.float64(x0+w))))\n",
    "                y1=min(img_init.shape[1],int(math.trunc(np.float64(y0+h))))\n",
    "                x_obj.append(np.copy(img_init[y0:y1,x0:x1]))\n",
    "    return x_obj,y_obj\n",
    "                \n",
    "def load_objects_as_binary_problem(imgs_path,max_samples=np.iinfo(np.int32)):\n",
    "    y_obj=[]\n",
    "    x_obj=[]\n",
    "    imgs_files = os.listdir( imgs_path+\"images/\" )\n",
    "    for i,img_file in enumerate(imgs_files):\n",
    "        if (len(x_obj)==max_samples):\n",
    "            break\n",
    "        label_file=img_file[:-4]+\".txt\"\n",
    "        #print(i,img_file,label_file)\n",
    "        img_init=cv2.imread(imgs_path+\"images/\"+img_file)\n",
    "        labels=csv.reader(open(imgs_path+\"labels/\"+label_file,\"r\"),delimiter=' ')\n",
    "        rows = list(labels)\n",
    "        if len(rows)>0:\n",
    "            for j,row in enumerate(rows):\n",
    "                if (len(x_obj)==max_samples):\n",
    "                    break\n",
    "                if (row[0]=='4'):\n",
    "                    y_obj.append(int(row[0]))\n",
    "                else:\n",
    "                    y_obj.append(0)\n",
    "                bbox=np.array(row[1:],dtype=np.float32) if row[0]==1 else np.array(row[1:],dtype=np.float32)\n",
    "                w=int(bbox[2]*img_init.shape[0])\n",
    "                h=int(bbox[3]*img_init.shape[1])\n",
    "                x0=max(0,int(math.trunc(np.float64(bbox[0]*img_init.shape[0]-w/2))))\n",
    "                y0=max(0,int(math.trunc(np.float64(bbox[1]*img_init.shape[1]-h/2))))\n",
    "                x1=min(img_init.shape[0],int(math.trunc(np.float64(x0+w))))\n",
    "                y1=min(img_init.shape[1],int(math.trunc(np.float64(y0+h))))\n",
    "                x_obj.append(img_init[y0:y1,x0:x1].copy())\n",
    "    return x_obj,y_obj\n",
    "\n",
    "def load_objects_as_binary_problem_balanced(imgs_path,max_positivesamples=np.iinfo(np.int32)):\n",
    "    return [],[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea594ee-4862-4bf4-abb6-e4e9e8e8a6a8",
   "metadata": {},
   "source": [
    "## Architectures légères\n",
    "\n",
    "Essayez des architectures simples combinant une ou deux couches de convolution et de pooling, suivies d'une ou deux couches denses.\n",
    "\n",
    "Pour la classification binaire, la dernière couche ne comporte qu'un seul neurone.\n",
    "\n",
    "Pour la classification multicatégorielle, la dernière couche comporte autant de neurones que de catégories. Le neurone gagnant est celui qui a la valeur de sortie la plus élevée. Un encodage des étiquettes catégorielles est nécessaire pour préparer les données d'entraînement (voir `tf.keras.utils.to_categorical`).\n",
    "\n",
    "Vous pouvez choisir le framework (PyTorch ou TensorFlow) avec lequel vous êtes à l'aise.\n",
    "\n",
    "Vous pouvez travailler en local ou sur Google Colab.\n",
    "\n",
    "Vous trouverez ci-dessous un exemple de code TensorFlow incomplet pour résoudre ce problème, mais vous pouvez le remplacer complétement par une solution de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484fe044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">246016</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">62,980,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,598</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_14 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m4,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_14 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m246016\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m62,980,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)             │         \u001b[38;5;34m3,598\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,988,814</span> (240.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m62,988,814\u001b[0m (240.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,988,814</span> (240.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m62,988,814\u001b[0m (240.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,864</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">246016</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">62,980,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_15 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m4,864\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_15 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_15 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m246016\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m62,980,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,985,473</span> (240.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m62,985,473\u001b[0m (240.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">62,985,473</span> (240.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m62,985,473\u001b[0m (240.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sklearn.cluster as skc\n",
    "import sklearn.svm as svm\n",
    "\n",
    "# multi classification \n",
    "\n",
    "x_train, y_train = load_objects(train_path, 1000)\n",
    "\n",
    "\n",
    "# Choose a target size\n",
    "TARGET_SIZE = (128, 128)\n",
    "\n",
    "def resize_images(images, target_size):\n",
    "    return [tf.image.resize(img, target_size) for img in images]\n",
    "\n",
    "x_train_resized = resize_images(x_train, TARGET_SIZE)\n",
    "x_train_tensor = tf.stack(x_train_resized)\n",
    "\n",
    "# preparing the network input tensor\n",
    "\n",
    "inputs = tf.keras.Input(shape=(*TARGET_SIZE, x_train_tensor.shape[-1]))\n",
    "\n",
    "# Convolutional Layer #1\n",
    "conv1 = tf.keras.layers.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"valid\",\n",
    "    activation=tf.nn.relu)(inputs)\n",
    "\n",
    "# Pooling Layer\n",
    "pool1 = tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=2)(conv1)\n",
    "\n",
    "# Flatten the pooling output\n",
    "flat = tf.keras.layers.Flatten()(pool1)\n",
    "\n",
    "# Dense layer\n",
    "mlp = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)(flat)\n",
    "\n",
    "# Output layer - number of units should match your number of classes\n",
    "y_train = [int(label) for label in y_train]\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Get number of classes\n",
    "num_classes = len(set(y_train))\n",
    "output_classif = tf.keras.layers.Dense(units=num_classes, activation='softmax')(mlp)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output_classif, )\n",
    "\n",
    "sparsecatloss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=sparsecatloss,  # Use categorical cross-entropy for multi-class classification\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "### binary classification \n",
    "\n",
    "# Load data\n",
    "x_train_bin, y_train_bin = load_objects_as_binary_problem(train_path, 1000)\n",
    "\n",
    "# Convert labels to integers (0 or 1)\n",
    "y_train_bin = np.array([int(label) for label in y_train_bin])\n",
    "\n",
    "# Resize images to fixed size\n",
    "x_train_bin_resized = [tf.image.resize(img, TARGET_SIZE) for img in x_train_bin]\n",
    "x_train_bin_tensor = tf.stack(x_train_bin_resized)\n",
    "\n",
    "# Build the model\n",
    "inputs_bin = tf.keras.Input(shape=(*TARGET_SIZE, x_train_bin_tensor.shape[-1]))\n",
    "\n",
    "conv1_bin = tf.keras.layers.Conv2D(\n",
    "    filters=64,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"valid\",\n",
    "    activation=tf.nn.relu)(inputs_bin)\n",
    "\n",
    "pool1_bin = tf.keras.layers.MaxPooling2D(pool_size=[2, 2], strides=2)(conv1_bin)\n",
    "\n",
    "flat_bin = tf.keras.layers.Flatten()(pool1_bin)\n",
    "\n",
    "mlp_bin = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)(flat_bin)\n",
    "\n",
    "output_bin = tf.keras.layers.Dense(units=1, activation='sigmoid')(mlp_bin)\n",
    "\n",
    "model_bin = tf.keras.Model(inputs=inputs_bin, outputs=output_bin)\n",
    "\n",
    "model_bin.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_bin.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f73e6b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 126ms/step - accuracy: 0.2500 - loss: 322.4623\n",
      "Epoch 2/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.6250 - loss: 12.4636\n",
      "Epoch 3/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.8820 - loss: 1.0278\n",
      "Epoch 4/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - accuracy: 0.9400 - loss: 0.3829\n",
      "Epoch 5/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - accuracy: 0.9670 - loss: 0.2838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x12fe7eb10>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_resized = resize_images(x_train, TARGET_SIZE)\n",
    "x_train_tensor = tf.stack(x_train_resized)\n",
    "\n",
    "y_train = [int(label) for label in y_train]\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "model.fit(x=x_train_tensor, y=y_train, batch_size=32, epochs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01472086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - accuracy: 0.8680 - loss: 38.9859\n",
      "Epoch 2/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - accuracy: 0.8230 - loss: 0.5685\n",
      "Epoch 3/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - accuracy: 0.9750 - loss: 0.3414\n",
      "Epoch 4/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.9760 - loss: 0.1502\n",
      "Epoch 5/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - accuracy: 0.9580 - loss: 0.0958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x12fd22030>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_bin, y_train_bin = load_objects_as_binary_problem(train_path, 1000)\n",
    "\n",
    "x_train_bin_resized = [tf.image.resize(img, TARGET_SIZE) for img in x_train_bin]\n",
    "x_train_bin_tensor = tf.stack(x_train_bin_resized)\n",
    "y_train_bin = np.array([int(label) for label in y_train_bin])\n",
    "\n",
    "model_bin.fit(x_train_bin_tensor, y_train_bin, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5486d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_multiclass.keras')\n",
    "model_bin.save('model_binary.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab1dd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Q2** Évaluez le modèle sur les ensembles d'entraînement et de test pour les deux modalités de classification (binaire et multiclasse).\n",
    "Précisez les performances en train et test pour les différentes configurations essayées. \n",
    "\n",
    "__Réponse__:\n",
    "\n",
    "\n",
    "**Q3** Que se passe-t-il si vous changez de modalité de classification ? Faut-il réentraîner entièrement le modèle ? Quelles parties peuvent être conservées ?\n",
    "\n",
    "__Réponse__ : Les parties de convolutions peuvent êtres conservées, la seule couche qu'il faut changer est la couche de MLP qui fait la classification \n",
    "\n",
    "**Q4** Avez-vous des remarques concernant la classification d'objets par BoW (TP2) par rapport à la classification d'objets par CNN ?\n",
    "\n",
    "__Réponse__ : C'est plus efficace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "795ef03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.8036 - loss: 2.0518\n",
      "Test Loss: 2.0518\n",
      "Test Accuracy: 0.8036\n"
     ]
    }
   ],
   "source": [
    "### Q3 Model evaluation multi classification \n",
    "\n",
    "# Load and preprocess test data\n",
    "x_test, y_test = load_objects(test_path, 1000)\n",
    "\n",
    "# Convert labels to integers\n",
    "y_test = np.array([int(label) for label in y_test])\n",
    "\n",
    "# Resize images (same as training)\n",
    "x_test_resized = [tf.image.resize(img, TARGET_SIZE) for img in x_test]\n",
    "x_test_tensor = tf.stack(x_test_resized)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test_tensor, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69236d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9418 - loss: 0.0645\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       795\n",
      "           1       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.94       825\n",
      "   macro avg       0.33      0.33      0.33       825\n",
      "weighted avg       0.95      0.94      0.95       825\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ainazeaze/etude/amvo/amvo/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/ainazeaze/etude/amvo/amvo/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/ainazeaze/etude/amvo/amvo/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/ainazeaze/etude/amvo/amvo/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/ainazeaze/etude/amvo/amvo/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/ainazeaze/etude/amvo/amvo/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "x_test_bin, y_test_bin = load_objects_as_binary_problem(test_path, 1000)\n",
    "y_test_bin = np.array([int(label) for label in y_test_bin])\n",
    "x_test_bin_resized = [tf.image.resize(img, TARGET_SIZE) for img in x_test_bin]\n",
    "x_test_bin_tensor = tf.stack(x_test_bin_resized)\n",
    "\n",
    "# Evaluate\n",
    "loss_bin, accuracy_bin = model_bin.evaluate(x_test_bin_tensor, y_test_bin)\n",
    "\n",
    "# Predictions\n",
    "y_pred_bin_probs = model_bin.predict(x_test_bin_tensor)\n",
    "y_pred_bin = (y_pred_bin_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_bin, y_pred_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73521f79",
   "metadata": {},
   "source": [
    "## Transfer learning (optionnel et pas nécessaire pour la suite)\n",
    "\n",
    "Dans cette partie, nous allons refaire le même exercice en réutilisant un modèle existant, tel que MobileNet, qui a déjà été entraîné.\n",
    "\n",
    "Une vaste collection de modèles pré-entraînés est disponible dans des frameworks comme TensorFlow.\n",
    "\n",
    "Le code ci-dessous utilise MobileNet, mais vous pouvez facilement changer de modèle en instanciant celui de votre choix.\n",
    "\n",
    "Pour cette partie, il est recommandé d'utiliser Google Colab ou Grid5K.\n",
    "Dans ce cas, vous devrez également envisager de télécharger les données sur votre Drive ou votre répertoire personnel Grid5K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prepare the data\n",
    "#the x_train should be ideally reshaped to (_,224,224,3)\n",
    "\n",
    "#from tensorflow.keras import layers, models\n",
    "#from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "\n",
    "\n",
    "#base_model=MobileNet(weights=\"imagenet\", include_top=False, input_shape=x_train[0].shape)\n",
    "#include_top is False in order to drop the default Dense Layers and to be able to add your own\n",
    "#base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "#flat=layers.Flatten()(base_model)\n",
    "#mlp=tf.keras.layers.Dense(units=256, activation=tf.nn.relu)(flatten_layer)\n",
    "#...\n",
    "#output_classif = ...(mlp)\n",
    "\n",
    "#model = tf.keras.Model(inputs=inputs, outputs=output_classif)\n",
    "\n",
    "#catloss=tf.keras.losses.CategoricalCrossentropy()\n",
    "#binloss=tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "#model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "#              loss=...,metrics=['accuracy'])\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fc178",
   "metadata": {},
   "source": [
    "**Q5** Évaluez le modèle sur les ensembles d'entraînement et de test pour les deux modalités de classification (binaire et multiclasse).\n",
    "Précisez les performances en train et test pour les différentes configurations essayées. \n",
    "\n",
    "__Réponse__:\n",
    "\n",
    "\n",
    "**Q6** Que se passe-t-il si vous changez de modalité de classification ? Faut-il réentraîner entièrement le modèle ? Quelles parties peuvent être conservées ?\n",
    "\n",
    "__Réponse__ :\n",
    "\n",
    "**Q7** Avez-vous des remarques concernant la classification d'objets par BoW (TP2) par rapport à la classification d'objets par CNN ?\n",
    "\n",
    "__Réponse__ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071bd19-6d41-4074-914c-95d5a5725b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188b4d8-27f2-4b2b-95d8-bb7274bab85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amvo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
